{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnonoPyhE-8d"
      },
      "source": [
        "# Custom 데이터로 CNN 모델 학습 실습\n",
        "## 모듈 선언"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUboaygMU-9X",
        "outputId": "3abbb283-ef6b-4e13-b44f-ab046f000b86"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgWsx2eBEuGD"
      },
      "source": [
        "import torch\n",
        "import torch.cuda as cuda\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms, datasets\n",
        "from torchvision.datasets import ImageFolder"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqleN0LhF4Qk"
      },
      "source": [
        "## cuda(device), seed 선언"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBfx5CznF9sd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af45b381-857e-4b33-f883-9a313285455a"
      },
      "source": [
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "torch.manual_seed(1)\n",
        "if device=='cuda':\n",
        "    torch.cuda.manual_seed_all(999)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFkGEXA2GL0j"
      },
      "source": [
        "## 데이터셋 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wY-5p-LXLkNq"
      },
      "source": [
        "class custom_dataset(Dataset):\n",
        "    def __init__(self, path, transfer):\n",
        "        data = ImageFolder(root=path, transform=transfer)\n",
        "        self.imgs = []\n",
        "        self.labels = []\n",
        "\n",
        "        for i, (img, label) in enumerate(data):\n",
        "            self.imgs.append(img)\n",
        "            self.labels.append([label])\n",
        "\n",
        "        self.length = len(self.labels)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "    \n",
        "    def __getitem__(self, item):\n",
        "        data = self.imgs[item]\n",
        "        target = torch.Tensor(self.labels[item]).to(torch.long).squeeze()\n",
        "        return data, target\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYGCn5HLGQHN"
      },
      "source": [
        "\n",
        "train_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.RandomCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                            std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                            std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "\n",
        "train_dataset = custom_dataset('/content/drive/MyDrive/대학원/스터디/학부생스터디/6주차/custom_dataset/train/', train_transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=100,\n",
        "                                           shuffle=True,)\n",
        "\n",
        "test_dataset = custom_dataset('/content/drive/MyDrive/대학원/스터디/학부생스터디/6주차/custom_dataset/test/', test_transform)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                           batch_size=100,\n",
        "                                           shuffle=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy7h4bIJFcOH"
      },
      "source": [
        "## 모델 선언"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE1tXOQCFff3"
      },
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.imagesize = (224,224,3)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 5, kernel_size=5, padding=2)      # 5,224,224\n",
        "        self.conv2 = nn.Conv2d(5, 10, kernel_size=5, padding=2)     # 10,224,224\n",
        "        self.conv3 = nn.Conv2d(10, 20, kernel_size=5, padding=2)    # 20,224,224\n",
        "        self.maxpool1 = nn.MaxPool2d(2)                             # 20,112,112\n",
        "\n",
        "        self.conv4 = nn.Conv2d(20, 20, kernel_size=3, padding=1)    # 20,112,112\n",
        "        self.conv5 = nn.Conv2d(20, 20, kernel_size=3, padding=1)    # 20,112,112\n",
        "        self.maxpool2 = nn.MaxPool2d(2)                             # 20,56,56\n",
        "\n",
        "        self.fc1 = nn.Linear(20 * 56 * 56, 512)\n",
        "        self.fc2 = nn.Linear(512, 4)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        in_size = x.size(0)\n",
        "\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.maxpool1(x))\n",
        "\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = F.relu(self.maxpool2(x))\n",
        "\n",
        "        x = x.view(in_size, -1) \n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)    \n",
        "\n",
        "        return x\n",
        "\n",
        "model = Net()\n",
        "model = model.to(device)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO2nnyNNGFxv"
      },
      "source": [
        "## 학습 시작"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkTsKs7BGKZl",
        "outputId": "85a12eb6-4ffc-467d-f6b6-3c6bf1a9ceea"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "Best = -1\n",
        "\n",
        "for epoch in range(1, 11):\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        train_loss += loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        output = model(data)\n",
        "        test_loss += criterion(output, target).item()\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    acc = 100. * correct / len(test_loader.dataset)\n",
        "    if acc > Best:\n",
        "        Best = acc\n",
        "        \n",
        "    print('Train Epoch {}: loss: {:.4f}'.format(epoch, loss))\n",
        "    print('Test :  loss: {:.4f}, Accuracy: {}/{} {:.0f}% \\n\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset), acc))\n",
        "print('Best_acc: ',Best.item())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch 1: loss: 0.0667\n",
            "Test :  loss: 0.6700, Accuracy: 87/100 87% \n",
            "\n",
            "\n",
            "Train Epoch 2: loss: 0.0075\n",
            "Test :  loss: 0.6326, Accuracy: 84/100 84% \n",
            "\n",
            "\n",
            "Train Epoch 3: loss: 0.0041\n",
            "Test :  loss: 0.7737, Accuracy: 80/100 80% \n",
            "\n",
            "\n",
            "Train Epoch 4: loss: 0.0022\n",
            "Test :  loss: 0.8004, Accuracy: 84/100 84% \n",
            "\n",
            "\n",
            "Train Epoch 5: loss: 0.0025\n",
            "Test :  loss: 0.8560, Accuracy: 83/100 83% \n",
            "\n",
            "\n",
            "Train Epoch 6: loss: 0.0025\n",
            "Test :  loss: 0.8792, Accuracy: 82/100 82% \n",
            "\n",
            "\n",
            "Train Epoch 7: loss: 0.0008\n",
            "Test :  loss: 0.9231, Accuracy: 80/100 80% \n",
            "\n",
            "\n",
            "Train Epoch 8: loss: 0.0007\n",
            "Test :  loss: 0.9447, Accuracy: 80/100 80% \n",
            "\n",
            "\n",
            "Train Epoch 9: loss: 0.0009\n",
            "Test :  loss: 0.9771, Accuracy: 80/100 80% \n",
            "\n",
            "\n",
            "Train Epoch 10: loss: 0.0009\n",
            "Test :  loss: 0.9992, Accuracy: 81/100 81% \n",
            "\n",
            "\n",
            "Best_acc:  87.0\n"
          ]
        }
      ]
    }
  ]
}